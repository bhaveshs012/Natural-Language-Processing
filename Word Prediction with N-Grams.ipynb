{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Prediction With N-Grams\n",
    "### By RUTUJA SHINDE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext=\"I love South Korea. I love Kpop . I love BTS .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'South', 'Korea', '.', 'I', 'love', 'Kpop', '.', 'I', 'love', 'BTS', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = nltk.word_tokenize(mytext)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution for Unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Korea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kpop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BTS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Words  Frequency\n",
       "0      I          3\n",
       "1   love          3\n",
       "2  South          1\n",
       "3  Korea          1\n",
       "4      .          3\n",
       "5   Kpop          1\n",
       "6    BTS          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist = nltk.FreqDist(tokens)\n",
    "#print(freq_dist)\n",
    "#print(freq_dist.most_common())                       \n",
    "#freq_dist.plot()\n",
    "import pandas as pd\n",
    "uni_df = pd.DataFrame(freq_dist.items(), columns= ['Words','Frequency'])\n",
    "uni_df\n",
    "#a=freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        I\n",
      "1     love\n",
      "2    South\n",
      "3    Korea\n",
      "4        .\n",
      "5     Kpop\n",
      "6      BTS\n",
      "Name: Words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "c=uni_df['Words']\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3\n",
      "1    3\n",
      "2    1\n",
      "3    1\n",
      "4    3\n",
      "5    1\n",
      "6    1\n",
      "Name: Frequency, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fre=uni_df['Frequency']\n",
    "print(fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        I\n",
      "1     love\n",
      "2    South\n",
      "3    Korea\n",
      "4        .\n",
      "5     Kpop\n",
      "6      BTS\n",
      "Name: Words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(uni_df['Words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('I', 3), ('love', 3), ('South', 1), ('Korea', 1), ('.', 3), ('Kpop', 1), ('BTS', 1)])\n"
     ]
    }
   ],
   "source": [
    "uni_dic=freq_dist.items()\n",
    "print(uni_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([3, 3, 1, 1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "uni_val=freq_dist.values()\n",
    "print(uni_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "len_tok= len(tokens)\n",
    "print(len_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequency for Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Words  Frequency\n",
      "0      I   0.230769\n",
      "1   love   0.230769\n",
      "2  South   0.076923\n",
      "3  Korea   0.076923\n",
      "4      .   0.230769\n",
      "5   Kpop   0.076923\n",
      "6    BTS   0.076923\n"
     ]
    }
   ],
   "source": [
    "uni_df['Frequency']=uni_df['Frequency']/len_tok\n",
    "print(uni_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution for Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(I, love)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(love, South)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(South, Korea)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Korea, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(., I)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(love, Kpop)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Kpop, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(love, BTS)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(BTS, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Words  Frequency\n",
       "0       (I, love)          3\n",
       "1   (love, South)          1\n",
       "2  (South, Korea)          1\n",
       "3      (Korea, .)          1\n",
       "4          (., I)          2\n",
       "5    (love, Kpop)          1\n",
       "6       (Kpop, .)          1\n",
       "7     (love, BTS)          1\n",
       "8        (BTS, .)          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create  bigrams \n",
    "bgs = nltk.bigrams(tokens)\n",
    "import pandas as pd\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "bi_df = pd.DataFrame(fdist.items(), columns= ['Words','Frequency'])\n",
    "bi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         .    I  love  South  Korea  Kpop  BTS\n",
      ".      0.0  0.0   0.0    0.0    1.0   1.0  1.0\n",
      "I      2.0  0.0   0.0    0.0    0.0   0.0  0.0\n",
      "love   0.0  3.0   0.0    0.0    0.0   0.0  0.0\n",
      "South  0.0  0.0   1.0    0.0    0.0   0.0  0.0\n",
      "Korea  0.0  0.0   0.0    1.0    0.0   0.0  0.0\n",
      "Kpop   0.0  0.0   1.0    0.0    0.0   0.0  0.0\n",
      "BTS    0.0  0.0   1.0    0.0    0.0   0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "import pandas as pd\n",
    "def generate_co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    " \n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    " \n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    " \n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    " \n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix, vocab_index\n",
    "import itertools\n",
    "# Create one list using many lists\n",
    "#data = list(itertools.chain.from_iterable(tokens))\n",
    "matrix, vocab_index = generate_co_occurrence_matrix(tokens)\n",
    " \n",
    " \n",
    "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
    "                             columns=vocab_index)\n",
    "print(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_index['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a=matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b=matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequency table for Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 importing libraries for calculating requency distribution tables for unigram and bigram.\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import string\n",
    "import time\n",
    "import gc\n",
    "from math import log10\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable declaration\n",
    "unigram_dict = defaultdict(int)          #for storing the different words with their frequencies    \n",
    "bi_dict = defaultdict(int)             #for keeping count of sentences of two words     \n",
    "bi_prob_dict = defaultdict(list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative frequency table for mytext words\n",
    "def relfreq(uni_dic, matrix, vocab_index):\n",
    "    for word,freq in uni_dic:\n",
    "        index=vocab_index[word]\n",
    "        #print(index.word,matrix[index]/freq)\n",
    "        matrix[index]=matrix[index]/freq\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Frequency Table:\n",
      "              .    I  love  South     Korea      Kpop       BTS\n",
      ".      0.000000  0.0   0.0    0.0  0.333333  0.333333  0.333333\n",
      "I      0.666667  0.0   0.0    0.0  0.000000  0.000000  0.000000\n",
      "love   0.000000  1.0   0.0    0.0  0.000000  0.000000  0.000000\n",
      "South  0.000000  0.0   1.0    0.0  0.000000  0.000000  0.000000\n",
      "Korea  0.000000  0.0   0.0    1.0  0.000000  0.000000  0.000000\n",
      "Kpop   0.000000  0.0   1.0    0.0  0.000000  0.000000  0.000000\n",
      "BTS    0.000000  0.0   1.0    0.0  0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rel_freq_table=relfreq(uni_dic,matrix, vocab_index)\n",
    "rel_freq_matrix = pd.DataFrame(rel_freq_table, index=vocab_index,\n",
    "                             columns=vocab_index)\n",
    "print(\"Relative Frequency Table:\")\n",
    "print(rel_freq_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization for Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_sums = rel_freq_matrix.sum(axis=1)\n",
    "new_matrix = rel_freq_matrix / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         .    I  love  South     Korea      Kpop       BTS\n",
      ".      0.0  0.0   0.0    0.0  0.333333  0.333333  0.333333\n",
      "I      1.0  0.0   0.0    0.0  0.000000  0.000000  0.000000\n",
      "love   0.0  1.0   0.0    0.0  0.000000  0.000000  0.000000\n",
      "South  0.0  0.0   1.0    0.0  0.000000  0.000000  0.000000\n",
      "Korea  0.0  0.0   0.0    1.0  0.000000  0.000000  0.000000\n",
      "Kpop   0.0  0.0   1.0    0.0  0.000000  0.000000  0.000000\n",
      "BTS    0.0  0.0   1.0    0.0  0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "print(new_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.33333333 0.33333333\n",
      "  0.33333333]\n",
      " [0.66666667 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "arr=np.asarray(matrix)                          #using numpy make array of the matrix elements\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_prediction\n",
    "import random\n",
    "def word_predict(matrix, vocab_index):\n",
    "    #print(matrix)\n",
    "    try:\n",
    "        word = input(\"Enter the word:\")\n",
    "        index=vocab_index[word]\n",
    "        arr=np.asarray(matrix)\n",
    "        #print(arr)\n",
    "        row=arr[:,index]\n",
    "        #print(row)\n",
    "        r=random.random()\n",
    "        print(r)\n",
    "        absol_diff=abs(row-r)\n",
    "#         print(absol_diff)\n",
    "        out_index=np.argmin(absol_diff)\n",
    "        print(out_index)\n",
    "        key_list = list(vocab_index.keys()) \n",
    "        val_list = list(vocab_index.values())\n",
    "        return key_list[val_list.index(out_index)]\n",
    "    except Exception:                                                     #handle the exception\n",
    "        return(\"NULL\")\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probability 'r' for the input word and predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word:love\n",
      "0.6931298243465666\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'South'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_predict(rel_freq_table, vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Word Prediction Function on 3 different cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word:Korea\n",
      "0.751577159262234\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_predict(rel_freq_table, vocab_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word:me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NULL'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_predict(rel_freq_table, vocab_index)\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word:BTS\n",
      "0.7233172356635524\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_predict(rel_freq_table, vocab_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As the random number is generated for the input word, the model with give random probabilities and hence the output for the next predicted word will change"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
